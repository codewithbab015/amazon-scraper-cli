{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83d\udd77\ufe0f Web Scraping Pipeline Project","text":""},{"location":"#project-setup","title":"\ud83d\udee0\ufe0f Project Setup","text":""},{"location":"#create-python-virtual-environment","title":"\ud83d\udc0d Create Python Virtual Environment","text":"<pre><code>python -m venv .venv\n</code></pre>"},{"location":"#activate-the-virtual-environment","title":"\u26a1 Activate the Virtual Environment","text":"<pre><code>source .venv/bin/activate\n</code></pre>"},{"location":"#install-required-python-packages","title":"\ud83d\udce6 Install Required Python Packages","text":"<pre><code># Upgrade pip\npython -m pip install --upgrade pip\n\n# Install packages from requirements.txt\npip install -r requirements.txt\n</code></pre>"},{"location":"#install-playwright-in-cli-environment","title":"\ud83c\udfad Install Playwright in CLI Environment","text":"<pre><code>playwright install firefox\n</code></pre>"},{"location":"#project-execution-guide","title":"\ud83d\ude80 Project Execution Guide","text":"<p>This project uses a modular web scraping pipeline, managed via a single entry-point script: <code>run_script.py</code>.</p> <p>Each pipeline run is configured through the <code>configs.yml</code> file, using the fields below:</p> Field Description <code>run_group</code> A logical group name for related scraping jobs <code>run_name</code> The specific scraping task within the group <code>run_type</code> The stage of the pipeline to execute (<code>extract</code>, <code>transform</code>, <code>load</code>)"},{"location":"#script-responsibilities","title":"\ud83e\udded Script Responsibilities","text":"<code>run_type</code> Script Responsibility <code>extract</code> <code>extract/scraper_extractor.py</code> Scrapes summary data from the target website and saves it to a JSON file <code>transform</code> <code>transform/scraper_transform.py</code> Loads the JSON from <code>extract</code>, scrapes detailed product data, and outputs another JSON <code>load</code> <code>load/scraper_load.py</code> Cleans the transformed data and saves the final output to a timestamped CSV"},{"location":"#pipeline-workflow-and-script-order","title":"\ud83d\udd01 Pipeline Workflow and Script Order","text":"<p>Scripts must be executed in sequence due to data dependencies:</p> <pre><code>Step 1: scraper_extractor.py   \u2192 Collect high-level metadata\nStep 2: scraper_transform.py   \u2192 Use Step 1 output to gather detailed records\nStep 3: scraper_load.py        \u2192 Clean Step 2 output and export as CSV\n</code></pre>"},{"location":"#running-the-pipeline","title":"\u25b6\ufe0f Running the Pipeline","text":"<p>To run the pipeline via CLI, it is recommended to use a Taskfile.</p> <ul> <li><code>MAX</code> maps to the <code>--max</code> CLI argument in <code>Taskfile.yml</code></li> <li>\u26a0\ufe0f Note: <code>MAX</code> is required only when <code>RUN_MODE</code> is set to <code>\"extract\"</code></li> <li>Python CLI arguments are centrally defined via the Taskfile for ease of use</li> </ul> <pre><code># Run the pipeline directly (for debugging or quick tests):\npython run_script.py \\\n  --run_group \"electronics\" \\\n  --run_name \"camera-photo\" \\\n  --run_mode \"extract\" \\\n  --max 1\n\n# Preferred Taskfile-based command:\ntask cli-runner:run \\\n  RUN_GROUP=\"electronics\" \\\n  RUN_NAME=\"camera-photo\" \\\n  RUN_MODE=\"extract\" \\\n  MAX=1\n</code></pre>"},{"location":"#run-pipeline-overview-cli","title":"\ud83e\udded Run Pipeline Overview (CLI)","text":"<pre><code>flowchart TD\n    %% Define styles for modern look\n    classDef startEnd fill:#e1f5fe,stroke:#01579b,stroke-width:2px,color:#000\n    classDef process fill:#f3e5f5,stroke:#4a148c,stroke-width:2px,color:#000\n    classDef decision fill:#fff3e0,stroke:#e65100,stroke-width:2px,color:#000\n    classDef success fill:#e8f5e8,stroke:#2e7d32,stroke-width:2px,color:#000\n    classDef error fill:#ffebee,stroke:#c62828,stroke-width:2px,color:#000\n\n    %% Main flow\n    A[\ud83d\ude80 Start] --&gt; B[\ud83d\udc64 User runs CLI command via Taskfile]\n    B --&gt; C[\ud83d\udccb Taskfile loads commands and variables from both RunCLI.yml and CommonVars.yml]\n    C --&gt; D{\ud83d\udd0d RUN_MODE == extract}\n    D --&gt;|\u2705 Yes| E[\ud83d\udd22 Check if MAX is provided]\n    D --&gt;|\u274c No| F[\u25b6\ufe0f Run without MAX]\n    E --&gt;|\ud83c\udfaf MAX provided| G[\u26a1 Execute run_script.py with all args]\n    E --&gt;|\u26a0\ufe0f MAX missing| H[\u274c Fail with error: MAX is required]\n    F --&gt; G\n    G --&gt; I[\ud83d\udcbe Cleaned data is saved to /data folder]\n    I --&gt; J[\ud83c\udf89 End]\n\n    %% Apply styles\n    class A,J startEnd\n    class B,C,F,G,I process\n    class D,E decision\n    class H error\n</code></pre>"},{"location":"#deployment-steps-recap","title":"\u2705 Deployment Steps (Recap)","text":"<pre><code># Step 1: Install MkDocs and Material theme\npip install mkdocs-material\n\n# Step 2: Serve locally\nmkdocs serve -a localhost:8001\n\n# Step 3: Deploy to GitHub Pages\nmkdocs gh-deploy\n</code></pre>"}]}